name: Scrape Costco Deals

on:
  schedule:
    - cron: '0 6 * * *'  # Daily at 6 AM UTC
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Costco Intel
        uses: actions/checkout@v4

      - name: Checkout DeadManUltimateScraper
        uses: actions/checkout@v4
        with:
          repository: DeadManOfficial/DeadManUltimateScraper
          path: scraper

      - name: Checkout token-optimization
        uses: actions/checkout@v4
        with:
          repository: DeadManOfficial/token-optimization
          path: token-optimization

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install TOR
        run: |
          sudo apt-get update
          sudo apt-get install -y tor
          sudo service tor start
          sleep 10
          curl --socks5-hostname 127.0.0.1:9050 https://check.torproject.org/api/ip || echo "TOR check failed"

      - name: Install dependencies
        run: |
          # Install token-optimization
          cd token-optimization && pip install -r requirements.txt || true && cd ..

          # Install DeadMan scraper deps
          cd scraper && pip install -e . --no-deps || true && cd ..
          pip install curl_cffi "httpx[socks]" beautifulsoup4 pydantic pyyaml stem tldextract fake-useragent requests aiohttp lxml PySocks

      - name: Test Scraper Infrastructure
        working-directory: ./scraper
        env:
          PYTHONPATH: "${{ github.workspace }}/token-optimization/src:${{ github.workspace }}/scraper"
        run: |
          python -c "
          import asyncio
          from deadman_scraper.core.config import Config
          from deadman_scraper.fetch.tor_manager import TORManager

          async def test():
              config = Config.from_env()
              tor = TORManager(config.tor)

              if await tor.start():
                  print(f'✅ TOR connected: {tor.proxy_url}')

                  from curl_cffi.requests import AsyncSession
                  async with AsyncSession(impersonate='chrome120', proxy=tor.proxy_url) as session:
                      # Test 1: httpbin (shows our TOR IP)
                      resp = await session.get('https://httpbin.org/ip', timeout=30)
                      print(f'✅ httpbin response: {resp.text.strip()}')

                      # Test 2: HackerNews API (public data)
                      resp2 = await session.get('https://hacker-news.firebaseio.com/v0/topstories.json', timeout=30)
                      stories = resp2.json()[:5]
                      print(f'✅ HackerNews top 5 story IDs: {stories}')

                      # Test 3: Reddit JSON (public)
                      resp3 = await session.get('https://www.reddit.com/r/deals/hot.json?limit=3', timeout=30)
                      if resp3.status_code == 200:
                          data = resp3.json()
                          posts = [p['data']['title'][:50] for p in data['data']['children'][:3]]
                          print(f'✅ Reddit r/deals posts: {posts}')
                      else:
                          print(f'⚠️ Reddit: {resp3.status_code}')
              else:
                  print('❌ TOR not available')

          asyncio.run(test())
          "

      - name: Run DeadMan Costco Scraper
        working-directory: ./scraper
        env:
          PYTHONPATH: "${{ github.workspace }}/token-optimization/src:${{ github.workspace }}/scraper"
        run: |
          python -c "
          import asyncio
          import sys
          sys.path.insert(0, '.')
          from deadman_scraper.scrapers.costco_scraper import scrape_all_keywords, save_deals

          async def main():
              deals = await scrape_all_keywords(['clearance', 'deals', 'sale'], limit=100)
              if deals:
                  save_deals(deals)
              else:
                  print('No deals found')

          asyncio.run(main())
          "

      - name: Copy deals to web
        run: |
          if [ -f scraper/web/src/data/deals.json ]; then
            cp scraper/web/src/data/deals.json web/src/data/deals.json
          elif [ -f web/src/data/deals.json ]; then
            echo "Using existing deals.json"
          fi

      - name: Check for changes
        id: check
        run: |
          if git diff --quiet web/src/data/deals.json 2>/dev/null; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "changed=true" >> $GITHUB_OUTPUT
          fi

      - name: Commit and push
        if: steps.check.outputs.changed == 'true'
        run: |
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git config user.name "github-actions[bot]"
          git add web/src/data/deals.json
          git commit -m "chore: Update deals [DeadMan Scraper]"
          git push

      - name: Summary
        run: |
          echo "### DeadMan Scraper Complete" >> "$GITHUB_STEP_SUMMARY"
          if [ "${{ steps.check.outputs.changed }}" == "true" ]; then
            echo "✅ New deals committed" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "ℹ️ No changes" >> "$GITHUB_STEP_SUMMARY"
          fi
